{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from uqdd.models.utils_models import load_model, get_model_config\n",
    "from uqdd.models.baseline import BaselineDNN\n",
    "from uqdd.models.evidential import EvidentialDNN\n",
    "from uqdd.models.mcdropout import run_mcdropout # we will use baseline with run_mcdp\n",
    "\n",
    "# METRICS\n",
    "from uqdd.models.utils_train import evaluate_predictions\n",
    "# from uqdd.utils import get_config\n",
    "from uqdd.utils import load_df\n",
    "from uqdd.models.utils_metrics import MetricsTable\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load csv runs and model names file\n",
    "runs_path = 'runs.csv'\n",
    "runs_df = load_df(runs_path)\n",
    "\n",
    "# Load predictions for each model_name\n",
    "# predictions_path = 'predictions.csv'\n",
    "preds_dirpath = '/home/bkhalil/Repos/uqdd/uqdd/data/predictions/papyrus/xc50/all/'\n",
    "filename = 'preds.csv'\n",
    "predictions_df = load_df(preds_dirpath+filename)\n"
   ],
   "id": "246628798747dc7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:19:42.062348Z",
     "start_time": "2024-07-02T16:19:41.938596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# get arrays from df cols\n",
    "y_true = predictions_df['y_true'].values\n",
    "y_pred = predictions_df['y_pred'].values\n",
    "y_err = predictions_df['y_err'].values\n",
    "y_alea = predictions_df['y_alea'].values\n",
    "eps_col = 'y_std' if 'y_std' in predictions_df.columns else 'y_eps'\n",
    "y_eps = predictions_df[eps_col].values\n",
    "\n",
    "# we need to get model_type and config from runs_df\n",
    "model_type = \"ensemble\"\n",
    "\n",
    "config = get_model_config(\n",
    "    model_type,\n",
    "    data_name=\"papyrus\",\n",
    "    activity_type=\"xc50\",\n",
    "    split_type=\"random\",\n",
    "    descriptor_protein=\"ankh-large\",\n",
    "    descriptor_chemical=\"ecfp2048\",\n",
    "    data_specific_path=\"papyrus/xc50/all\"\n",
    ")\n",
    "\n",
    "\n",
    "uct_metrics_logger = MetricsTable(\n",
    "    model_type=model_type,\n",
    "    config=config,\n",
    "    add_plots_to_table=False,\n",
    "    # * we can turn on if we want to see them in wandb * #\n",
    "    # logger=logger,\n",
    "    project_name=None, # to change as this will become the output csv file name\n",
    "    run_name=None, # that would be the same run name from the runs.csv file\n",
    ")\n",
    "task_name = \"PCM\"\n",
    "metrics, plots = uct_metrics_logger(\n",
    "        y_pred=y_pred,\n",
    "        y_std=y_alea,\n",
    "        y_true=y_true,\n",
    "        y_err=y_err,\n",
    "        # y_alea=y_alea,\n",
    "        y_eps=y_eps,\n",
    "        task_name=task_name,\n",
    "        figpath=None, # Here we need to define the fig path \n",
    "        # the figpath default was FIGS_DIR / data_specific_path / self.model_name\n",
    "        # we can change it to mark the reassessment of the model somehoe (maybe add a prefix to model_name like reasses)\n",
    "        # FIGS_DIR is imported variable from uqdd\n",
    "        # data_specific_path is predefined var data_specific_path=\"papyrus/xc50/all\"\n",
    "    )\n",
    "\n",
    "# * calculate metrics for a subset of the datapoints * #\n",
    "submetrics, subplots = uct_metrics_logger(\n",
    "    y_pred=y_pred,\n",
    "    y_std=y_alea,\n",
    "    y_true=y_true,\n",
    "    y_err=y_err,\n",
    "    # y_alea=y_alea,\n",
    "    y_eps=y_eps,\n",
    "    task_name=task_name+\" Subset\",\n",
    "    n_subset=100,\n",
    ")\n",
    "\n",
    "# then we log to the csv file which appends after each iteration\n",
    "uct_metrics_logger.csv_log()\n"
   ],
   "id": "cdc72a19f926c3af",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# get arrays from df cols\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m y_true \u001B[38;5;241m=\u001B[39m predictions_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m      3\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m predictions_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_pred\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m      4\u001B[0m y_err \u001B[38;5;241m=\u001B[39m predictions_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_err\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n",
      "\u001B[0;31mNameError\u001B[0m: name 'predictions_df' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "79b3b44e52f0f600"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
